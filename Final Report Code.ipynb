{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3WQkSf0a9kV",
        "outputId": "305d674c-ef6b-43ce-b691-ff3465f8cee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "\n",
        "print(\"True.csv hits:\", glob.glob('/content/drive/MyDrive/**/True.csv', recursive=True)[:10])\n",
        "print(\"Fake.csv hits:\", glob.glob('/content/drive/MyDrive/**/Fake.csv', recursive=True)[:10])\n",
        "\n",
        "TRUE_PATH = '/content/drive/MyDrive/True.csv'\n",
        "FAKE_PATH = '/content/drive/MyDrive/Fake.csv'\n",
        "\n",
        "print(\"TRUE exists? \", os.path.exists(TRUE_PATH), \"->\", TRUE_PATH)\n",
        "print(\"FAKE exists? \", os.path.exists(FAKE_PATH), \"->\", FAKE_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ccxcb64plPL5",
        "outputId": "5678ed34-c144-4f6e-daef-717bbe16ec07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True.csv hits: ['/content/drive/MyDrive/True.csv']\n",
            "Fake.csv hits: ['/content/drive/MyDrive/Fake.csv']\n",
            "TRUE exists?  True -> /content/drive/MyDrive/True.csv\n",
            "FAKE exists?  True -> /content/drive/MyDrive/Fake.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "TRUE_CSV = Path(TRUE_PATH)\n",
        "FAKE_CSV = Path(FAKE_PATH)\n",
        "\n",
        "assert TRUE_CSV.exists(), f\"Missing: {TRUE_CSV}\"\n",
        "assert FAKE_CSV.exists(), f\"Missing: {FAKE_CSV}\"\n",
        "\n",
        "print(\"Using:\", TRUE_CSV, FAKE_CSV)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC2EvTfWlych",
        "outputId": "e04c6dfa-f996-4864-d01b-05bdac225658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: /content/drive/MyDrive/True.csv /content/drive/MyDrive/Fake.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports and basic settings**:"
      ],
      "metadata": {
        "id": "OUb-TN5brYbq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsU1TeBTD7z-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load ISOT Fake/True dataset:**"
      ],
      "metadata": {
        "id": "1bvNX9BUruUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_isot(fake_path, true_path):\n",
        "    fake_df = pd.read_csv(fake_path)\n",
        "    true_df = pd.read_csv(true_path)\n",
        "\n",
        "    fake_df[\"label\"] = \"fake\"\n",
        "    true_df[\"label\"] = \"true\"\n",
        "\n",
        "    # Combine two dataframes\n",
        "    df = pd.concat([fake_df, true_df], ignore_index=True)\n",
        "\n",
        "    # One text column = title + body\n",
        "    df[\"text_full\"] = df[\"title\"].fillna(\"\") + \" \" + df[\"text\"].fillna(\"\")\n",
        "\n",
        "    # Drop exact duplicate articles\n",
        "    df = df.drop_duplicates(subset=[\"title\", \"text\"])\n",
        "\n",
        "    # Keep only the columns we care about now\n",
        "    df = df[[\"text_full\", \"label\", \"subject\", \"date\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# use the Path objects we already created\n",
        "df = load_isot(FAKE_CSV, TRUE_CSV)\n",
        "\n",
        "print(df.head())\n",
        "print(\"\\nLabel counts:\")\n",
        "print(df[\"label\"].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5F_JXQujcpn",
        "outputId": "a205b316-6d8b-4e3c-df37-2da1a8d5f226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           text_full label subject  \\\n",
            "0   Donald Trump Sends Out Embarrassing New Year’...  fake    News   \n",
            "1   Drunk Bragging Trump Staffer Started Russian ...  fake    News   \n",
            "2   Sheriff David Clarke Becomes An Internet Joke...  fake    News   \n",
            "3   Trump Is So Obsessed He Even Has Obama’s Name...  fake    News   \n",
            "4   Pope Francis Just Called Out Donald Trump Dur...  fake    News   \n",
            "\n",
            "                date  \n",
            "0  December 31, 2017  \n",
            "1  December 31, 2017  \n",
            "2  December 30, 2017  \n",
            "3  December 29, 2017  \n",
            "4  December 25, 2017  \n",
            "\n",
            "Label counts:\n",
            "label\n",
            "true    21197\n",
            "fake    17908\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train / validation / test split:**"
      ],
      "metadata": {
        "id": "NRmnBGXOr5zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[\"text_full\"].values\n",
        "y = df[\"label\"].values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train 70% vs temp 30%\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# Validation 15% vs Test 15%\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(X_train))\n",
        "print(\"Valid size:\", len(X_valid))\n",
        "print(\"Test size:\", len(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOqJmq8JoQib",
        "outputId": "c5974207-8668-41e8-de6a-b6afc1897b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 27373\n",
            "Valid size: 5866\n",
            "Test size: 5866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing and vocabulary:**"
      ],
      "metadata": {
        "id": "TczsjbjysCL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "TOKEN_PATTERN = re.compile(r\"[a-z]+\")   # regex pattern to extract alphabetic tokens only (a–z)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Lowercase, remove URLs, keep alphabetic tokens only.\n",
        "    \"\"\"\n",
        "    text = text.lower()   # convert entire text to lowercase for consistency\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)   # remove URLs and replace them with spaces\n",
        "    tokens = TOKEN_PATTERN.findall(text)   # extract alphabetic tokens using the regex pattern\n",
        "    return tokens    # return list of cleaned tokens\n",
        "\n",
        "\n",
        "def build_vocabulary(texts, min_freq=5):\n",
        "    \"\"\"\n",
        "    Build a word -> index dictionary from training texts.\n",
        "    Only keep words that appear at least min_freq times.\n",
        "    \"\"\"\n",
        "    freq = {}   # dictionary to count token frequencies across all training texts\n",
        "\n",
        "    for t in texts:\n",
        "        tokens = preprocess_text(t)   # preprocess each document\n",
        "        for tok in tokens:\n",
        "            freq[tok] = freq.get(tok, 0) + 1   # increment count or initialize to 1\n",
        "\n",
        "    vocab = {}   # final vocabulary mapping token -> index\n",
        "    idx = 0      # index counter\n",
        "    for word, count in freq.items():\n",
        "        if count >= min_freq:     # keep only words that occur enough times\n",
        "            vocab[word] = idx     # assign index to the word\n",
        "            idx += 1              # increment index for next word\n",
        "\n",
        "    return vocab   # return constructed vocabulary dictionary\n",
        "\n",
        "\n",
        "def vectorize_tokens(tokens, vocab):\n",
        "    \"\"\"\n",
        "    Convert list of tokens into a count vector.\n",
        "    \"\"\"\n",
        "    vec = np.zeros(len(vocab), dtype=np.int32)   # initialize vector of zeros, length = vocab size\n",
        "    for tok in tokens:\n",
        "        if tok in vocab:         # check if token exists in vocabulary\n",
        "            j = vocab[tok]       # get index of the token\n",
        "            vec[j] += 1          # increment count in the corresponding position\n",
        "    return vec    # return the bag-of-words count vector\n",
        "\n",
        "\n",
        "# Build vocabulary from the training set\n",
        "vocab = build_vocabulary(X_train, min_freq=5)   # construct vocabulary using training data\n",
        "print(\"Vocabulary size:\", len(vocab))           # print number of unique tokens kept\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHgQMN81oi74",
        "outputId": "ccba1a99-4eae-4c7b-a751-d5d705ad363e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 35464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From-scratch Multinomial Naive Bayes:**"
      ],
      "metadata": {
        "id": "U7FCWv-2sKyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialNBScratch:\n",
        "    \"\"\"\n",
        "    Simple Multinomial Naive Bayes for text classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha                      # Laplace smoothing parameter\n",
        "        self.classes_ = None                    # stores the class labels (e.g., [\"fake\", \"true\"])\n",
        "        self.class_priors_ = None               # P(class) for each class\n",
        "        self.feature_log_prob_ = None           # log P(word | class) matrix\n",
        "        self.vocab_ = None                      # reference to the vocabulary used\n",
        "\n",
        "    def fit(self, texts, labels, vocab):\n",
        "        \"\"\"\n",
        "        Train the Naive Bayes model on the given texts and labels.\n",
        "        \"\"\"\n",
        "        self.vocab_ = vocab                     # save the vocabulary\n",
        "        n_docs = len(texts)                     # number of documents in the training set\n",
        "\n",
        "        # find unique classes and map labels to numeric indices\n",
        "        self.classes_, y_indices = np.unique(labels, return_inverse=True)\n",
        "        n_classes = len(self.classes_)          # number of classes (2 in our case)\n",
        "        n_features = len(vocab)                 # size of vocabulary\n",
        "\n",
        "        # initialize word count matrix [num_classes x vocab_size]\n",
        "        class_word_counts = np.zeros((n_classes, n_features), dtype=np.int64)\n",
        "        class_doc_counts = np.zeros(n_classes, dtype=np.int64)   # document count per class\n",
        "\n",
        "        # loop over every training document\n",
        "        for i, text in enumerate(texts):\n",
        "            c_idx = y_indices[i]                # index of the class for this document\n",
        "            class_doc_counts[c_idx] += 1        # count how many docs belong to each class\n",
        "\n",
        "            tokens = preprocess_text(text)      # preprocess text into tokens\n",
        "            vec = vectorize_tokens(tokens, vocab)   # convert tokens into a count vector\n",
        "            class_word_counts[c_idx] += vec     # add counts to this class's word totals\n",
        "\n",
        "        # compute prior probabilities P(class)\n",
        "        self.class_priors_ = class_doc_counts / n_docs\n",
        "\n",
        "        # apply Laplace smoothing to word counts\n",
        "        alpha = self.alpha\n",
        "        smoothed = class_word_counts + alpha\n",
        "\n",
        "        # sum of word counts for each class (needed for normalization)\n",
        "        class_totals = smoothed.sum(axis=1).reshape(-1, 1)\n",
        "\n",
        "        # compute conditional probabilities P(word | class) in log-space\n",
        "        self.feature_log_prob_ = np.log(smoothed / class_totals)\n",
        "\n",
        "    def _log_posterior(self, text):\n",
        "        \"\"\"\n",
        "        Compute log posterior probabilities for each class for a single document.\n",
        "        \"\"\"\n",
        "        tokens = preprocess_text(text)                 # preprocess text\n",
        "        vec = vectorize_tokens(tokens, self.vocab_)    # convert to count vector\n",
        "\n",
        "        # log likelihood: sum(count * log P(word|class)) over all words\n",
        "        log_likelihood = (self.feature_log_prob_ * vec).sum(axis=1)\n",
        "\n",
        "        # log prior: log(P(class))\n",
        "        log_prior = np.log(self.class_priors_)\n",
        "\n",
        "        # log posterior for each class\n",
        "        return log_prior + log_likelihood\n",
        "\n",
        "    def predict(self, texts):\n",
        "        \"\"\"\n",
        "        Predict class labels for a list of texts.\n",
        "        \"\"\"\n",
        "        preds = []\n",
        "        for t in texts:\n",
        "            log_post = self._log_posterior(t)          # compute log posteriors\n",
        "            c_idx = np.argmax(log_post)                # choose class with highest score\n",
        "            preds.append(self.classes_[c_idx])         # map index back to class label\n",
        "        return np.array(preds)\n"
      ],
      "metadata": {
        "id": "j-97VQaoo6gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Small subset experiment:**"
      ],
      "metadata": {
        "id": "LLW6w5NJsSz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_subset_size = 1000\n",
        "test_subset_size = 300\n",
        "\n",
        "X_train_small = X_train[:train_subset_size]\n",
        "y_train_small = y_train[:train_subset_size]\n",
        "X_test_small = X_test[:test_subset_size]\n",
        "y_test_small = y_test[:test_subset_size]\n",
        "\n",
        "vocab_small = build_vocabulary(X_train_small, min_freq=3)\n",
        "print(\"Small vocab size:\", len(vocab_small))\n",
        "\n",
        "nb_small = MultinomialNBScratch(alpha=1.0)\n",
        "nb_small.fit(X_train_small, y_train_small, vocab_small)\n",
        "\n",
        "y_pred_small = nb_small.predict(X_test_small)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "acc_small = accuracy_score(y_test_small, y_pred_small)\n",
        "print(f\"Subset accuracy (scratch NB) = {acc_small:.4f}\")\n",
        "\n",
        "print(\"\\nClassification report (subset):\")\n",
        "print(classification_report(y_test_small, y_pred_small))\n",
        "\n",
        "cm_small = confusion_matrix(y_test_small, y_pred_small, labels=[\"fake\", \"true\"])\n",
        "print(\"Confusion matrix (rows=actual, cols=pred, order=[fake, true]):\\n\", cm_small)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCo1o8pppeYW",
        "outputId": "11d256a3-c5c2-41eb-d6b8-28bd2159e68c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small vocab size: 9752\n",
            "Subset accuracy (scratch NB) = 0.9533\n",
            "\n",
            "Classification report (subset):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.94      0.96      0.95       139\n",
            "        true       0.97      0.94      0.96       161\n",
            "\n",
            "    accuracy                           0.95       300\n",
            "   macro avg       0.95      0.95      0.95       300\n",
            "weighted avg       0.95      0.95      0.95       300\n",
            "\n",
            "Confusion matrix (rows=actual, cols=pred, order=[fake, true]):\n",
            " [[134   5]\n",
            " [  9 152]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Full train experiment:**"
      ],
      "metadata": {
        "id": "eYp2GRU2sYSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_full = build_vocabulary(X_train, min_freq=5)\n",
        "print(\"Full vocab size:\", len(vocab_full))\n",
        "\n",
        "nb_full = MultinomialNBScratch(alpha=1.0)\n",
        "nb_full.fit(X_train, y_train, vocab_full)\n",
        "\n",
        "y_pred_test = nb_full.predict(X_test)\n",
        "acc_test = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Test accuracy (scratch NB) = {acc_test:.4f}\")\n",
        "print(\"\\nClassification report (full test):\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "cm_full = confusion_matrix(y_test, y_pred_test, labels=[\"fake\", \"true\"])\n",
        "print(\"Confusion matrix (rows=actual, cols=pred, order=[fake, true]):\\n\", cm_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAqhkrwEp20l",
        "outputId": "dd25a1b8-77e6-4361-afb7-7ea7546dd879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full vocab size: 35464\n",
            "Test accuracy (scratch NB) = 0.9535\n",
            "\n",
            "Classification report (full test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.95      0.95      0.95      2687\n",
            "        true       0.96      0.96      0.96      3179\n",
            "\n",
            "    accuracy                           0.95      5866\n",
            "   macro avg       0.95      0.95      0.95      5866\n",
            "weighted avg       0.95      0.95      0.95      5866\n",
            "\n",
            "Confusion matrix (rows=actual, cols=pred, order=[fake, true]):\n",
            " [[2552  135]\n",
            " [ 138 3041]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2: Logistic Regression with TF-IDF features\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1. Build TF-IDF features from training text and apply to val/test\n",
        "tfidf = TfidfVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words='english',   # you can remove this if you don't want stopword removal\n",
        "    min_df=5,               # ignore very rare words\n",
        "    ngram_range=(1, 2)      # unigrams + bigrams\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_valid_tfidf = tfidf.transform(X_valid)\n",
        "X_test_tfidf  = tfidf.transform(X_test)\n",
        "\n",
        "print(\"TF-IDF shapes:\")\n",
        "print(\"  train:\", X_train_tfidf.shape)\n",
        "print(\"  valid:\", X_valid_tfidf.shape)\n",
        "print(\"  test :\", X_test_tfidf.shape)\n",
        "\n",
        "# 2. Train Logistic Regression classifier\n",
        "log_reg = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "log_reg.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 3. Evaluate on validation set\n",
        "y_valid_pred = log_reg.predict(X_valid_tfidf)\n",
        "print(\"\\nValidation performance (Logistic Regression):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_valid, y_valid_pred))\n",
        "print(classification_report(y_valid, y_valid_pred))\n",
        "\n",
        "# 4. Final evaluation on test set\n",
        "y_test_pred = log_reg.predict(X_test_tfidf)\n",
        "print(\"\\nTest performance (Logistic Regression):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "print(\"Confusion matrix (test):\")\n",
        "print(confusion_matrix(y_test, y_test_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UWlXDq_xHm0",
        "outputId": "e960d15e-defb-42f3-8ef8-208faa4445c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF shapes:\n",
            "  train: (27373, 179817)\n",
            "  valid: (5866, 179817)\n",
            "  test : (5866, 179817)\n",
            "\n",
            "Validation performance (Logistic Regression):\n",
            "Accuracy: 0.9856801909307876\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.99      0.98      0.98      2686\n",
            "        true       0.98      0.99      0.99      3180\n",
            "\n",
            "    accuracy                           0.99      5866\n",
            "   macro avg       0.99      0.99      0.99      5866\n",
            "weighted avg       0.99      0.99      0.99      5866\n",
            "\n",
            "\n",
            "Test performance (Logistic Regression):\n",
            "Accuracy: 0.9858506648482782\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.99      0.98      0.98      2687\n",
            "        true       0.98      0.99      0.99      3179\n",
            "\n",
            "    accuracy                           0.99      5866\n",
            "   macro avg       0.99      0.99      0.99      5866\n",
            "weighted avg       0.99      0.99      0.99      5866\n",
            "\n",
            "Confusion matrix (test):\n",
            "[[2626   61]\n",
            " [  22 3157]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 3: Linear SVM (LinearSVC) with the same TF-IDF features\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm_clf = LinearSVC()\n",
        "\n",
        "# 1. Train on training TF-IDF features\n",
        "svm_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 2. Evaluate on validation set\n",
        "y_valid_pred_svm = svm_clf.predict(X_valid_tfidf)\n",
        "print(\"\\nValidation performance (LinearSVC):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_valid, y_valid_pred_svm))\n",
        "print(classification_report(y_valid, y_valid_pred_svm))\n",
        "\n",
        "# 3. Final evaluation on test set\n",
        "y_test_pred_svm = svm_clf.predict(X_test_tfidf)\n",
        "print(\"\\nTest performance (LinearSVC):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred_svm))\n",
        "print(classification_report(y_test, y_test_pred_svm))\n",
        "\n",
        "print(\"Confusion matrix (test, LinearSVC):\")\n",
        "print(confusion_matrix(y_test, y_test_pred_svm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEJKojDvDpAa",
        "outputId": "21d47860-a160-4bc6-9f25-757f7541231f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation performance (LinearSVC):\n",
            "Accuracy: 0.9938629389703375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       1.00      0.99      0.99      2686\n",
            "        true       0.99      1.00      0.99      3180\n",
            "\n",
            "    accuracy                           0.99      5866\n",
            "   macro avg       0.99      0.99      0.99      5866\n",
            "weighted avg       0.99      0.99      0.99      5866\n",
            "\n",
            "\n",
            "Test performance (LinearSVC):\n",
            "Accuracy: 0.9933515172178656\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       1.00      0.99      0.99      2687\n",
            "        true       0.99      1.00      0.99      3179\n",
            "\n",
            "    accuracy                           0.99      5866\n",
            "   macro avg       0.99      0.99      0.99      5866\n",
            "weighted avg       0.99      0.99      0.99      5866\n",
            "\n",
            "Confusion matrix (test, LinearSVC):\n",
            "[[2655   32]\n",
            " [   7 3172]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Model 4: RNN (Bidirectional LSTM) on raw text\n",
        "# ============================================================\n",
        "\n",
        "# 2. Tokenize the raw text and create padded sequences\n",
        "\n",
        "MAX_VOCAB = 30000   # max number of words in the vocabulary\n",
        "MAX_LEN   = 300     # max number of tokens per article\n",
        "\n",
        "# Fit tokenizer on training text ONLY\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convert text -> integer sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_valid_seq = tokenizer.texts_to_sequences(X_valid)\n",
        "X_test_seq  = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad / truncate to fixed length\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "X_valid_pad = pad_sequences(X_valid_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "X_test_pad  = pad_sequences(X_test_seq,  maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "print(\"Padded shapes:\")\n",
        "print(\"  train:\", X_train_pad.shape)\n",
        "print(\"  valid:\", X_valid_pad.shape)\n",
        "print(\"  test :\", X_test_pad.shape)\n",
        "\n",
        "\n",
        "# 1. Encode labels (\"fake\" / \"true\") as 0 / 1 for Keras\n",
        "label_to_id = {\"fake\": 0, \"true\": 1}\n",
        "\n",
        "y_train_int = np.array([label_to_id[y] for y in y_train])\n",
        "y_valid_int = np.array([label_to_id[y] for y in y_valid])\n",
        "y_test_int  = np.array([label_to_id[y] for y in y_test])\n",
        "\n",
        "print(\"Label encoding check (train):\", np.bincount(y_train_int))\n",
        "\n",
        "\n",
        "# 3. Build the Bidirectional LSTM model\n",
        "\n",
        "vocab_size    = min(MAX_VOCAB, len(tokenizer.word_index) + 1)\n",
        "embedding_dim = 100\n",
        "\n",
        "rnn_model = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        input_length=MAX_LEN\n",
        "    ),\n",
        "    Bidirectional(LSTM(64, return_sequences=False)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation=\"sigmoid\")   # binary output\n",
        "])\n",
        "\n",
        "rnn_model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "rnn_model.summary()\n",
        "\n",
        "\n",
        "# 4. Train the RNN with early stopping on the validation set\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = rnn_model.fit(\n",
        "    X_train_pad, y_train_int,\n",
        "    validation_data=(X_valid_pad, y_valid_int),\n",
        "    epochs=4,\n",
        "    batch_size=128,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 5. Evaluate on the test set and compare with NB / Logistic / SVM\n",
        "\n",
        "test_loss, test_acc = rnn_model.evaluate(X_test_pad, y_test_int, verbose=0)\n",
        "print(f\"Test accuracy (RNN, BiLSTM) = {test_acc:.4f}\")\n",
        "\n",
        "y_prob = rnn_model.predict(X_test_pad).ravel()\n",
        "y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\nClassification report (RNN, BiLSTM):\")\n",
        "print(classification_report(y_test_int, y_pred, target_names=[\"fake\", \"true\"]))\n",
        "\n",
        "print(\"Confusion matrix (RNN, BiLSTM):\")\n",
        "print(confusion_matrix(y_test_int, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "_AnNJ2n3AJnh",
        "outputId": "1c7bfa3e-3c98-479d-ac7b-5fb6571e815a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padded shapes:\n",
            "  train: (27373, 300)\n",
            "  valid: (5866, 300)\n",
            "  test : (5866, 300)\n",
            "Label encoding check (train): [12535 14838]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 1s/step - accuracy: 0.8605 - loss: 0.2819 - val_accuracy: 0.9986 - val_loss: 0.0043\n",
            "Epoch 2/4\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 1s/step - accuracy: 0.9997 - loss: 0.0029 - val_accuracy: 0.9988 - val_loss: 0.0051\n",
            "Epoch 3/4\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 1s/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 0.9980 - val_loss: 0.0076\n",
            "Test accuracy (RNN, BiLSTM) = 0.9980\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 91ms/step\n",
            "\n",
            "Classification report (RNN, BiLSTM):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       1.00      1.00      1.00      2687\n",
            "        true       1.00      1.00      1.00      3179\n",
            "\n",
            "    accuracy                           1.00      5866\n",
            "   macro avg       1.00      1.00      1.00      5866\n",
            "weighted avg       1.00      1.00      1.00      5866\n",
            "\n",
            "Confusion matrix (RNN, BiLSTM):\n",
            "[[2679    8]\n",
            " [   4 3175]]\n"
          ]
        }
      ]
    }
  ]
}